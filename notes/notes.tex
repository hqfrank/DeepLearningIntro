\documentclass[]{article}

\usepackage{amsfonts}
%opening
\title{Notes on Deep Learning}
\author{Qiang Hu}

\begin{document}

\maketitle

\begin{abstract}
In this note, the concepts and knowledge on Deep Learning are recorded. 
\end{abstract}

\section{Introduction to Deep Learning}
\begin{itemize}
	\item Broadly speaking, deep learning is a more approachable name for an artificial neural network. The \textit{deep} in deep learning refers to the depth of the network. An artificial neural network can be very shallow.
	\item The basic level of a neural network is the \textbf{perceptron}, the mathematical representation of a biological neuron. There are several layers of interconnected perceptrons in a neural network.
	\item \textbf{Deep learning} focuses on using \textbf{neural networks} for complex practical problems.
	\item Deep neural networks are used for \textit{object recognition} and \textit{image analysis}, for various modules of self-driving cars, for chatbots and natural language understanding problems.
	\item Linear models and stochastic optimization methods are crucial for training deep neural networks.
	\item Popular building blocks of neural networks includes \textbf{fully connected layers}, \textit{convolutional} and \textit{recurrent layers}. These blocks are used to define complex modern architectures in \textbf{TensorFlow} and \textbf{Keras} frameworks.
	\item Prerequisite knowledge:
	\begin{itemize}
		\item \textbf{Machine learning} involves computer intelligence that doesn't know the answers up front. Instead, the program will run against training data, verify the success of its attempts, and modify its approach accordingly. Machine learning typically requires a sophisticated education, spanning software engineering and computer science to statistical methods and linear algebra.
		\item With \textbf{unsupervised learning}, there aren't any predefined or corresponding answers. The goal is to figure out the \textit{hidden patterns} in the data. It's usually used for \textbf{clustering} and \textbf{associative tasks}, like grouping customers by behavior. Amazon's ``customers who also bought..." recommendations are a type of associative task.
		\item Deep learning has proven to be an effective unsupervised learning technique. \textit{Computer vision} has been a main beneficiary of deep learning. \textit{Speech recognition} is another area that is felt deep learning's impact, because spoken languages are so vast and ambiguous. Through deep learning, generalizing the two languages (e.g., English and Mandarin) didn't require much additional design effort (i.e., design very different features).
		\item \textbf{Linear Regression:} a supervised machine learning algorithm where the predicted output is continuous and has a constant slope.
		\item \textbf{Mean Squared Error} (MSE): as a cost function, it is used to optimize the weights. MSE measures the average squared difference between an observation's actual and predicted values. The output is a single number representing the cost, or score, associated with our current set of weights.
		\begin{equation}
		MSE = \frac{1}{N} \sum_{i=1}^{n}{(y_i-y^`_i)^2}
		\end{equation}
		$y_i$ is the actual value of an observation, and $y^`_i$ is the prediction.
		\item \textbf{Gradient descent}: a method to minimize MSE. Calculates the gradient of the cost function. To capture the impact of each weight on the final prediction, we use \textbf{partial derivatives}. To find the partial derivatives, we use the \textbf{Chain rule}. E.g.,
		\begin{equation}
		f^`(m,b)=\left[
		\begin{array}{c}
		\frac{df}{dm} \\
		\frac{df}{db}
		\end{array}
		\right]
		\end{equation}
		To minimize the cost function, we iteratively move in the direction of steepest descent as defined by the negative of the gradient.
		
		\item \textbf{Learning rate}: it is size of the update on the weights. The calculated gradient tells us the slope of our cost function at our current position (i.e., weights) and the direction we should update to reduce our cost function. We should move in the direction opposite the gradient.
		
		\item \textbf{Training}: training a model is the process of iteratively improving your prediction equation by looping through the dataset multiple times, each time updating the weights values in the direction indicated by the slope of the cost function. Training is completed when we reach an acceptable error threshold, or when subsequent training iterations fail to reduce our cost. Before training, we need to initializing our weights (set default values), set our \textbf{hyperparameters} (i.e., learning rate and number of iterations).
		
		The optimization problem can be formed as,
		\begin{equation}
		\min_{w}L(w)=\min_{w}{\sum_{i=1}^{l}L(w;x_i,y_i)}
		\end{equation}
		L(w) is the loss (cost) function.\\
		$w^0$ is the initial weights.\\
		while True,
		\begin{equation}
		w^t=w^{t-1}-\eta_t\nabla{L(w^{t-1})}
		\end{equation}
		if $||w^t-w^{t-1}||<\epsilon$, break
		
		Note that, $l$ gradients should be computed on each step. If the dataset doesn`t fit in memory, it should be read from the disk on every GD step.
		
		\item \textbf{Normalization}: As the number of features grows, calculating gradient takes longer to compute. We can speed this up by ``normalizing" our input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Our goal now will be to normalize our features so they are all in the range -1 to 1.
		First, subtract the mean of the column (mean normalization); second, divide by the range of the column (feature scaling).
		
		\item \textbf{Logistic regression}: a classification algorithm used to assign observations to a discrete set of classes. Logistic regression transforms its output using the logistic sigmoid function to return a \textit{probability} value which can then be mapped to two or more discrete classes.
		
		\item \textbf{Sigmoid activation}: it is used to map predicted values to probabilities. The function maps any real value into another value between 0 and 1.
		\begin{equation}
		S(z)=\frac{1}{1+e^{-z}}
		\end{equation}
		It's easy to calculate the derivative of the sigmoid function.
		\begin{equation}
		s`(z)=s(z)(1-s(z))
		\end{equation}
		
		\item \textbf{Decision boundary}: the threshold value or tipping point used to map the probability value to a discrete class.
		
		\item \textbf{Prediction function}: in logistic regression, a prediction function returns the probability of our observation being positive, True, or ``Yes". We call this class 1 and its notation is $P(class=1)$. As the probability gets closer to 1, our model is more confident that the observation is in class 1.
		
		\item \textbf{Cross-Entropy}: a.k.a., \textbf{Log Loss}. It can be divided into two separate cost functions: one for $y=1$ and one for $y=0$.
		\begin{equation}
		\begin{array}{cc}
		J(\theta)=\frac{1}{m}{\sum_{i=1}^{m}{Cost(h_{\theta}(x^{(i)}),y^{(i)})}} &  \\
		Cost(h_{\theta}(x),y) = -\log{h_{\theta}(x)} & \mathrm{if} y=1 \\
		Cost(h_{\theta}(x),y) = -\log{(1-h_{\theta}(x))} & \mathrm{if} y=0
		\end{array}
		\end{equation}
		The key thing to note is the cost function penalizes confident and wrong predictions more than it rewards confident and right predictions!
		\begin{equation}
		J(\theta)=-\frac{1}{m}{\sum_{i=1}^{m}{y^{(i)}\log{(h_{\theta}(x^{(i)})})}+(1-y^{(i)})\log{(1-h_{\theta}(x^{(i)}))}}
		\end{equation}
		And the derivative of the cost function is:
		\begin{equation}
		C`=x\cdot {(s(z)-y)}
		\end{equation}

		\item \textbf{Multiclass logistic regression}: When $y=0,1,2,...,n$,
		\begin{enumerate}
			\item Divide the problem into $n+1$ binary classification problems;
			\item For each class ...
			\item Predict the probability the observations are in that single class.
			\item prediction$=\max{}$(probability of the classes)
		\end{enumerate}
		
		\item \textbf{Softmax transform}: transfer the score to probability.
		\begin{equation}
		z=(w_1^Tx, ..., w_K^Tx)\Rightarrow (e^{z_1}, ..., e^{z_K})
		\end{equation}
		\begin{equation}
		\sigma (z)=\left(\frac{e^{z_1}}{\sum_{k=1}^{K}e^{z_k}}, ...,\frac{e^{z_K}}{\sum_{k=1}^{K}e^{z_k}} \right)
		\end{equation}
		
		\item \textbf{Model regularization}: add a regularizer $R(w)$ to the original loss function $L(w)$. The regularizer penalizes the model for large weights. $\lambda$ is the regularization strength which controls the model quality on a training set and model complexity.
		\begin{equation}
		L_{\mathrm{reg}}(w)=L(w)+\lambda R(w)\Rightarrow \min_{w}{L_{\mathrm{reg}}(w)}
		\end{equation}
		To minimize the regularized loss function $L_{\mathrm{reg}}(w)$, it equals to the optimization problem,
		\begin{equation}
		\{
		\begin{array}{c}
		\min_{w}{L(w)} \\
		s.t.\ R(w) \leq C
		\end{array}
		\end{equation}
		If the L1 penalty $||w||=\sum{|w_i|}$ is used,
		\begin{itemize}
			\item Drives some weights exactly to zero;
			\item Learns sparse models;
			\item Cannot be optimized with simple gradient methods.
		\end{itemize}
		If the L2 penalty $||w||^2=\sum{w_j^2}$ is used,
		\begin{itemize}
			\item Drives all weights closer to zero;
			\item Can be optimized with gradient methods.
		\end{itemize}
		Other regularization techniques:
		\begin{itemize}
			\item Dimensionality reduction, e.g., remove features, principal component analysis;
			\item Data augmentation, e.g., upon image data, flip, rotate the image to get more data;
			\item Dropout;
			\item Early stopping;
			\item Collect more data.
		\end{itemize}
		
		\item \textbf{Stochastic gradient descent}: Similar to the regular GD, the training starts with an initial $w_0$, but in every step, we randomly choose an example with index $i$ between 1 and $l$ to update the weight. This process leads to very noisy approximations. But if we make enough numbers of iterations, it converges to some minimum.
		\begin{itemize}
			\item Noisy updates lead to fluctuations;
			\item Needs only one example on each step;
			\item Can be used in online setting;
			\item Learning rate $\eta_t$ should be chosen very carefully.
		\end{itemize}
		
		\item \textbf{Mini-batch gradient descent}: Similar to the stochastic GD, but in very step, we randomly choose $m$ examples from the dataset to calculated the gradient.
		\begin{itemize}
			\item Still can be used in online setting;
			\item Reduces the variance of gradient approximations;
			\item Learning rate $\eta_t$ should be chosen very carefully.
		\end{itemize}
		
		\item \textbf{Gradient descent extensions}: to optimize GD for difficult functions with complex level sets, we can define the \textbf{momentum} function $h_t=\alpha h_{t-1}+\eta_tg_t$, where $g_t$ is the gradient calculated. Then we update the weight using $h_t$, i.e., $w^t=w^{t-1}-h_t$. 
		\begin{itemize}
			\item It tends to move in the same direction as on previous steps;
			\item $h_t$ accumulates values along dimensions where gradients have the same sign;
			\item Usually, $\alpha=0.9$.
		\end{itemize}
		\textbf{Nesterov momentum}: $h_t=\alpha h_{t-1} + \eta_t \nabla L(w^{t-1}-\alpha h_{t-1})$
		
		\item \textbf{AdaGrad}: choose learning rate adaptively.
		\begin{equation}
		\begin{array}{c}
		G_j^t=G_j^{t-1}+g_{t,j}^2\\
		w_j^t=w_j^{t-1}-\frac{\eta_t g_{t,j}}{\sqrt{G_j^t+\epsilon}}
		\end{array}
		\end{equation}
		where $g_{t,j}$ is the gradient with respect to $j$-th parameter.
		\begin{itemize}
			\item Separate learning rates for each dimension;
			\item Suits for sparse data
			\item Learning rate can be fixed as $\eta_t=0.01$;
			\item $G_j^t$ always increases, leads to early stops.
		\end{itemize}
		
		\item \textbf{RMSprop}:
		\begin{equation}
		\begin{array}{c}
		G_j^t=\alpha G_j^{t-1}+ (1-\alpha) g_{t,j}^2\\
		w_j^t=w_j^{t-1}-\frac{\eta_t g_{t,j}}{\sqrt{G_j^t+\epsilon}}
		\end{array}
		\end{equation}
		\begin{itemize}
			\item $\alpha$ is about 0.9;
			\item Learning rate adapts to latest gradient steps.
		\end{itemize}
		
		\item \textbf{Adam}:
		\begin{equation}
		\begin{array}{c}
		m_j^t=\frac{\beta_1 m_j^{t-1}+(1-\beta_1)g_{t,j}}{1-\beta_1^t}\\
		v_j^t=\frac{\beta_2 v_j^{t-1}+(1-\beta_2)g_{t,j}^2}{1-\beta_2^t}\\
		w_j^t=w_j^{t-1}-\frac{\eta_t m_{j}^t}{\sqrt{v_j^t}+\epsilon}
		\end{array}
		\end{equation}
		Note in the first step, the normalization is very large, but as the iteration goes on, the denominator is close to 1. This is used to remove the bias of $v$ closing to 0 at the beginning. It also combines momentum and individual learning rates.
	\end{itemize}
\end{itemize}

\section{Introduction to neural networks}
\begin{itemize}
	\item In image processing, linear models are no helpful. Because pixel representations are dependencies, pixel representation in the class is very complex and very valuable.
	\item Transform the objects into some space in which the problem is easy.
	\item The old method to do image processing is that human experts construct features from lines and edges, which are combined into simple parts of image, into features, which are again, combined into discrete choices.
	\item Directly stacking multiple layers of liner models does not help the non-linear classification, because it still is linear models in use.
	\item Applying the output of the previous linear model to a non-linear function introduces the non-linearity. Some examples of non-linear functions:
	\begin{equation}
	f(a)=\frac{1}{1+e^a}
	\end{equation}
	\begin{equation}
	f(a)=\max{(0,a)}
	\end{equation}
	\begin{equation}
	f(a)=\tanh{(a)}
	\end{equation}
	\begin{equation}
	f(a) = \log{(1+e^a)}
	\end{equation}
	Most of them increase monotonically. And most of them are constant when the argument approaches $-\infty$. The choice usually depends on the network architecture.
	
	\item \textbf{Layer} is a building block for neural network:
	\begin{itemize}
		\item Input layer: input data;
		\item Dense layer: $f(x)=Wx+b$ (linear regressors);
		\item Non-linearity layer: $f(x)=\sigma(x)$;
		\item A few more: ...
		\item Output layer
	\end{itemize}
	All layers except input and output layers are \textit{hidden layers}.
	There are generally no connections between nodes in the same layer. 
	
	\item \textbf{Activation} is the output of a layer. (i.e., some intermediate signal in the neural network) The neurons in the hidden layers are only activated by nodes in the previous layer.
	
	\item Deep model is just a (composite) function.
	\begin{itemize}
		\item $f_1(f_2(f_3(x,w_3),w_2),w_1)$ $\rightarrow$ loss
		\item $x$ is feature vector
		\item $w_{1,2,3}$ are model parameters. 
	\end{itemize}

	\item The differentiation of the above deep model function is a \textit{Jacobian matrix} when $X=[x_1,x_2,...,x_n]$, $f=[f_1,f_2,...,f_m]$:
	\begin{equation}
	\frac{df}{dx}=\left[\frac{df}{dx_1}...\frac{df}{dx_n}\right]=\left[\begin{array}{ccc}
	\frac{df_1}{dx_1}&...&\frac{df_1}{dx_n}\\
	...&...&...\\
	\frac{df_m}{dx_1}&...&\frac{df_m}{dx_n}
	\end{array}\right]
	\end{equation}
	
	\item When $F:\mathbb{R}^{m\times n}\rightarrow \mathbb{R}^{p\times q}$, $X$ is a $m\times n$ matrix, $F$ is also a $p\times q$ matrix. Thus, $F`$ 
	\begin{itemize}
		\item a matrix of matrices
		\item fourth-rank tensor
		\item four-dimensional array
	\end{itemize} 
	\begin{equation}
	\frac{\partial F}{\partial X}=\left[\begin{array}{ccc}
	\frac{\partial F}{\partial X_{11}}&...&\frac{\partial F}{\partial X_{1n}}\\
	...&...&...\\
	\frac{\partial F}{\partial X_{m1}}&...&\frac{\partial F}{\partial X_{mn}}
	\end{array}\right]
	\end{equation}
	\begin{equation}
	\frac{\partial F}{\partial X_{ij}}=\left[\begin{array}{ccc}
	\frac{\partial F_{11}}{\partial X_{ij}}&...&\frac{\partial F_{1q}}{\partial X_{ij}}\\
	...&...&...\\
	\frac{\partial F_{p1}}{\partial X_{ij}}&...&\frac{\partial F_{pq}}{\partial X_{ij}}
	\end{array}\right]
	\end{equation}
\end{itemize}

\end{document}
